{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics - Stacking Layers\n",
    "\n",
    "By [Akshaj Verma](https://akshajverma.com/)\n",
    "\n",
    "This notebook takes you through the 4 ways of stacking layers in a neural net namely, `nn.Sequential`, `nn.Module`, `nn.ModuleList`, and `nn.ModuleDict`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Module` vs `nn.Functional`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has 2 major ways of defining layers. We can either use `nn.Module` or `nn.Functional`. `nn.Module` is the object oriented way of defining an architecture while `nn.Functional` is the functional way.  \n",
    "\n",
    "The major difference between the two is the ability to maintain a state. In case of a layer such as convolutional or a feedforward layer which has trainable parameters in the form of weights and biases, we use the object oriented way because we need to track those (parameters). While in the case of layers such as dropout and activation functions, which do not have any trainable parameters, we go for the functional way. Note that, we can still define things like dropout or activations using the object oriented way. \n",
    "\n",
    "`nn.Module` is stateful while `nn.Functional` is stateless. `nn.Module` uses `nn.Functional` under the hood for lot of operations such as [activation functions](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py) with the added advantage of initilializing and maintaining parameters for you. \n",
    "\n",
    "\n",
    "For more detailed information about nn.Module vs nn.Functional, have a look at [this answer](https://discuss.pytorch.org/t/beginner-should-relu-sigmoid-be-called-in-the-init-method/18689/6).\n",
    "\n",
    "\n",
    "`torch.nn.Module` => [What is torch.nn?](https://pytorch.org/tutorials/beginner/nn_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's dig into the different between `nn.Module`, `nn.Sequential`, `nn.ModuleList`, and `nn.ModuleDict` and how to use them. We will take the example of a CNN here BECAUSE WHY NOT. \n",
    "\n",
    "Task: Assume we are creating a neural net for image classification where the input is of the shape `[28 x 28 x 1]`. (*MNIST much?*)\n",
    "\n",
    "We will define a network with with minimum PyTorch abstractions and gradually make it better and better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `nn.Module`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way of defining a neural net in PyTorch is using the `nn.Module`. Let's do that now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModuleClassifier, self).__init__()\n",
    "        \n",
    "        # Block 1\n",
    "        self.conv_layer_1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.batchnorm_layer_1 = nn.BatchNorm2d(num_features=16)\n",
    "        self.dropout_layer_1 = nn.Dropout2d(p=0.1)\n",
    "        self.maxpool_layer_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv_layer_2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.batchnorm_layer_2 = nn.BatchNorm2d(32)\n",
    "        self.dropout_layer_2 = nn.Dropout2d(0.2)\n",
    "        self.maxpool_layer_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Block 3\n",
    "        self.conv_layer_3 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.batchnorm_layer_3 = nn.BatchNorm2d(64)\n",
    "        self.dropout_layer_3 = nn.Dropout2d(0.3)\n",
    "        self.maxpool_layer_3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "\n",
    "        self.relu_layer = nn.ReLU()\n",
    "        self.fc = nn.Linear(9*64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = self.conv_layer_1(x)\n",
    "        x = self.batchnorm_layer_1(x)\n",
    "        x = self.maxpool_layer_1(x)\n",
    "        x = self.dropout_layer_1(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv_layer_2(x)\n",
    "        x = self.batchnorm_layer_2(x)\n",
    "        x = self.maxpool_layer_2(x)\n",
    "        x = self.dropout_layer_2(x)\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.batchnorm_layer_3(x)\n",
    "        x = self.maxpool_layer_3(x)\n",
    "        x = self.dropout_layer_3(x)\n",
    "        \n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleClassifier(\n",
      "  (conv_layer_1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (batchnorm_layer_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout_layer_1): Dropout2d(p=0.1, inplace=False)\n",
      "  (maxpool_layer_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv_layer_2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (batchnorm_layer_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout_layer_2): Dropout2d(p=0.2, inplace=False)\n",
      "  (maxpool_layer_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv_layer_3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (batchnorm_layer_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout_layer_3): Dropout2d(p=0.3, inplace=False)\n",
      "  (maxpool_layer_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_layer): ReLU()\n",
      "  (fc): Linear(in_features=576, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_with_module = ModuleClassifier()\n",
    "print(model_with_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with the above code is apparent. It would be super cumbersome to add new layers with this method because  we'll have to initialize a layer in the `__init__()` method and use it in the `forward()` method. \n",
    "\n",
    "\n",
    "Let's use Sequential to mitigate some of the issues in the section below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SequentialClassifier, self).__init__()\n",
    "        \n",
    "        # Block 1\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        # Block 2\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        # Block 3\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.fc = nn.Linear(9*64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentialClassifier(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout2d(p=0.1, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout2d(p=0.2, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout2d(p=0.2, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=576, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_with_sequential = SequentialClassifier()\n",
    "print(model_with_sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` has enabled us to abstract away repeating layers into chunks of code. Not to mention, there's significantly less code in the `forward()` method. \n",
    "\n",
    "\n",
    "In the above code, we're still repeating a lot of code by defining and calling all the different lays. If you look closely, all three chunks contain similar layers with different values; namely - `nn.Conv2d` => `BatchNorm2d` => `nn.Dropout` => `nn.ReLU` => `nn.MaxPool2d`. We can abstract away this in a method which returns a `nn.Sequential` object. Let's see how to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterSequentialClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(BetterSequentialClassifier, self).__init__()\n",
    "        \n",
    "        # Block 1\n",
    "        self.block1 = self.cnn_chunk(channel_ip=1, channel_op=16, dropout_val=0.1, \n",
    "                                     kernel_size=5, stride=1, padding=2\n",
    "                                    )\n",
    "        \n",
    "        # Block 2\n",
    "        self.block2 = self.cnn_chunk(channel_ip=16, channel_op=32, dropout_val=0.2, \n",
    "                                     kernel_size=5, stride=1, padding=2\n",
    "                                    )        \n",
    "        \n",
    "        # Block 3\n",
    "        self.block3 = self.cnn_chunk(channel_ip=32, channel_op=64, dropout_val=0.3, \n",
    "                                     kernel_size=5, stride=1, padding=2\n",
    "                                    )\n",
    "        \n",
    "        self.fc = nn.Linear(9*64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def cnn_chunk(self, channel_ip, channel_op, dropout_val, **kwargs):\n",
    "        \n",
    "        # Yes, chonker like a fat cat. \n",
    "        chonker = nn.Sequential(\n",
    "            nn.Conv2d(channel_ip, channel_op, **kwargs),\n",
    "            nn.BatchNorm2d(channel_op),\n",
    "            nn.Dropout2d(dropout_val),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        return chonker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BetterSequentialClassifier(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout2d(p=0.1, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout2d(p=0.2, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout2d(p=0.3, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=576, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "better_model_with_sequential = BetterSequentialClassifier()\n",
    "print(better_model_with_sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take it up a notch. Let's define a classifier based on the following architecture where p=padding, k=kernel, s=stride.\n",
    "\n",
    "\n",
    "`Input` - `(28, 28, 1)` ==========> \n",
    "\n",
    "`p=2,k=3,s=1` - `Conv1 + BatchNorm` - `(28, 28, 8)`\n",
    "\n",
    "`p=3,k=5,s=1` - `Conv2 + Dropout` - `(28, 28, 16)`\n",
    "\n",
    "`p=0,k=2,s=3` - `Maxpool` - `(14, 14, 16)`\n",
    "\n",
    "`p=2,k=3,s=1` - `Conv3 + BatchNorm` - `(14, 14, 32)`\n",
    "\n",
    "`p=2,k=3,s=1` - `Conv4 + Dropout` - `(14, 14, 64)` ==========> `FC` - `(14 * 14 * 64, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvenBetterSequentialClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(EvenBetterSequentialClassifier, self).__init__()\n",
    "        \n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            self.conv_batchnorm(channel_ip=1, channel_op=8, padding=2, kernel_size=3, stride=1),\n",
    "            self.conv_dropout(channel_ip=8, channel_op=16,  dropout_val=0.1, padding=3, kernel_size=5, stride=1)\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            self.conv_batchnorm(channel_ip=16, channel_op=32, padding=2, kernel_size=3, stride=1),\n",
    "            self.conv_dropout(channel_ip=32, channel_op=64, dropout_val=0.2, padding=2, kernel_size=3, stride=1)\n",
    "        )\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=3)\n",
    "        self.linear = nn.Linear(in_features=14*14*64, out_features=num_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.block2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        out = self.linear(x)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "   \n",
    "    def conv_batchnorm(self, channel_ip, channel_op, **kwargs):\n",
    "        chonker = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channel_ip, out_channels=channel_op, **kwargs),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(channel_op)\n",
    "        )\n",
    "        \n",
    "        return chonker\n",
    "    \n",
    "    \n",
    "    def conv_dropout(self, channel_ip, channel_op, dropout_val, **kwargs):\n",
    "        chonker = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channel_ip, out_channels=channel_op, **kwargs),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout_val)\n",
    "        )\n",
    "        \n",
    "        return chonker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvenBetterSequentialClassifier(\n",
      "  (block1): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(3, 3))\n",
      "      (1): ReLU()\n",
      "      (2): Dropout2d(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "      (1): ReLU()\n",
      "      (2): Dropout2d(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear): Linear(in_features=12544, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "even_better_model_with_sequential = EvenBetterSequentialClassifier()\n",
    "print(even_better_model_with_sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` is a powerful tool to use as you've seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `nn.ModuleList`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModuleList helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
